{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Contextualized Token Embeddings: A Step-by-Step Guide\n",
    "\n",
    "This document explains key concepts in **word embeddings** using **GloVe, BERT, and CrossEncoder models**, along with code snippets for implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Import Required Libraries**\n",
    "```python\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "```\n",
    "### **Explanation**\n",
    "- `warnings.filterwarnings('ignore')`: Suppresses unnecessary warnings.\n",
    "- `numpy`: Used for numerical operations.\n",
    "- `matplotlib.pyplot`: Used for visualization.\n",
    "- `sklearn.decomposition.PCA`: Used for reducing dimensions in vector embeddings.\n",
    "- `torch`: PyTorch library for deep learning models.\n",
    "- `transformers`: Contains pre-trained NLP models like BERT.\n",
    "- `sklearn.metrics.pairwise.cosine_similarity`: Measures the similarity between vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. GloVe Word Embeddings**\n",
    "```python\n",
    "import gensim.downloader as api\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "#word_vectors = api.load('word2vec-google-news-300')\n",
    "```\n",
    "### **Explanation**\n",
    "- `gensim.downloader.api.load('glove-wiki-gigaword-100')`: Downloads **GloVe embeddings** (100-dimensional).\n",
    "- `word2vec-google-news-300`: Alternative model (300-dimensional Word2Vec embeddings).\n",
    "\n",
    "```python\n",
    "word_vectors['king'].shape\n",
    "word_vectors['king'][:20]  # Displays first 20 values of 'king' embedding\n",
    "```\n",
    "- Each word is represented as a **vector of numbers** capturing its meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Visualizing Word Embeddings with PCA**\n",
    "```python\n",
    "words = [\"king\", \"princess\", \"monarch\", \"throne\", \"crown\",\n",
    "         \"mountain\", \"ocean\", \"tv\", \"rainbow\", \"cloud\", \"queen\"]\n",
    "\n",
    "vectors = np.array([word_vectors[word] for word in words])\n",
    "```\n",
    "### **Explanation**\n",
    "- Defines a list of words.\n",
    "- Retrieves word embeddings for each word.\n",
    "\n",
    "```python\n",
    "pca = PCA(n_components=2)\n",
    "vectors_pca = pca.fit_transform(vectors)\n",
    "```\n",
    "- `PCA(n_components=2)`: Reduces the dimensions of word embeddings to 2D for visualization.\n",
    "\n",
    "```python\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "axes.scatter(vectors_pca[:, 0], vectors_pca[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    axes.annotate(word, (vectors_pca[i, 0]+.02, vectors_pca[i, 1]+.02))\n",
    "axes.set_title('PCA of Word Embeddings')\n",
    "plt.show()\n",
    "```\n",
    "- Plots words in **2D space** using PCA.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Word2Vec Algebra (Word Analogies)**\n",
    "```python\n",
    "result = word_vectors.most_similar(positive=['king', 'woman'],\n",
    "                                   negative=['man'], topn=1)\n",
    "```\n",
    "- Finds the word that best fits **\"king - man + woman\"**, which is expected to be **\"queen\"**.\n",
    "\n",
    "```python\n",
    "print(f\"\"\"\n",
    "    The word closest to 'king' - 'man' + 'woman' is: '{result[0][0]}'\n",
    "    with a similarity score of {result[0][1]}\n",
    "\"\"\")\n",
    "```\n",
    "- Displays the result with similarity score.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. GloVe vs. BERT: Words in Context**\n",
    "```python\n",
    "tokenizer = BertTokenizer.from_pretrained('./models/bert-base-uncased')\n",
    "model = BertModel.from_pretrained('./models/bert-base-uncased')\n",
    "```\n",
    "- Loads **BERT tokenizer and model**.\n",
    "\n",
    "```python\n",
    "def get_bert_embeddings(sentence, word):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    word_tokens = tokenizer.tokenize(sentence)\n",
    "    word_index = word_tokens.index(word)\n",
    "    word_embedding = last_hidden_states[0, word_index + 1, :]\n",
    "    return word_embedding\n",
    "```\n",
    "### **Explanation**\n",
    "- Tokenizes the sentence.\n",
    "- Extracts **last hidden state** from BERT.\n",
    "- Finds the embedding of the specified word (`bat`).\n",
    "\n",
    "```python\n",
    "sentence1 = \"The bat flew out of the cave at night.\"\n",
    "sentence2 = \"He swung the bat and hit a home run.\"\n",
    "word = \"bat\"\n",
    "\n",
    "bert_embedding1 = get_bert_embeddings(sentence1, word).detach().numpy()\n",
    "bert_embedding2 = get_bert_embeddings(sentence2, word).detach().numpy()\n",
    "word_embedding = word_vectors[word]\n",
    "```\n",
    "- Gets embeddings for **\"bat\"** in two different contexts.\n",
    "\n",
    "```python\n",
    "print(\"BERT Embedding for 'bat' in sentence 1:\", bert_embedding1[:5])\n",
    "print(\"BERT Embedding for 'bat' in sentence 2:\", bert_embedding2[:5])\n",
    "print(\"GloVe Embedding for 'bat':\", word_embedding[:5])\n",
    "```\n",
    "- Displays first **5 values** of embeddings.\n",
    "\n",
    "```python\n",
    "bert_similarity = cosine_similarity([bert_embedding1], [bert_embedding2])[0][0]\n",
    "word_embedding_similarity = cosine_similarity([word_embedding], [word_embedding])[0][0]\n",
    "```\n",
    "- Computes **cosine similarity** between embeddings.\n",
    "\n",
    "```python\n",
    "print(f\"Cosine Similarity between BERT embeddings in different contexts: {bert_similarity}\")\n",
    "print(f\"Cosine Similarity between GloVe embeddings: {word_embedding_similarity}\")\n",
    "```\n",
    "- Compares **BERT vs. GloVe** in capturing word meanings.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Cross Encoder for Sentence Similarity**\n",
    "```python\n",
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512,\n",
    "                     default_activation_function=torch.nn.Sigmoid())\n",
    "```\n",
    "- Loads a **CrossEncoder** model for **sentence similarity scoring**.\n",
    "\n",
    "```python\n",
    "question = \"Where is the capital of France?\"\n",
    "answers = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Madrid is the capital of Spain.\"\n",
    "]\n",
    "```\n",
    "- Defines a **question** and **candidate answers**.\n",
    "\n",
    "```python\n",
    "scores = model.predict([(question, answers[0]), (question, answers[1]),\n",
    "                        (question, answers[2])])\n",
    "```\n",
    "- Computes **similarity scores** between question and each answer.\n",
    "\n",
    "```python\n",
    "most_relevant_idx = torch.argmax(torch.tensor(scores)).item()\n",
    "print(f\"The most relevant passage is: {answers[most_relevant_idx]}\")\n",
    "```\n",
    "- Identifies the most **relevant answer**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "- **GloVe**: Provides fixed word embeddings.\n",
    "- **BERT**: Provides **contextualized** embeddings.\n",
    "- **PCA**: Used for visualization.\n",
    "- **CrossEncoder**: Finds best answers based on similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
